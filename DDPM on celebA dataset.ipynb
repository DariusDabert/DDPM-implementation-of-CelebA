{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3882543c",
      "metadata": {},
      "source": [
        "We used the following notebook to re implement the DDPM architecure:\n",
        "\n",
        "https://colab.research.google.com/drive/1AZ2_BAwXrU8InE_qAE9cFZ0lsIO5a_xp?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7db0dd78",
      "metadata": {},
      "source": [
        "## Standard import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "934767ee",
      "metadata": {
        "id": "934767ee"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision.transforms import Compose, ToTensor, Lambda, Resize\n",
        "from torchvision.models import inception_v3\n",
        "from scipy.linalg import sqrtm\n",
        "import zipfile\n",
        "import shutil\n",
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61474316",
      "metadata": {},
      "source": [
        "## Model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "713de2f5",
      "metadata": {
        "id": "713de2f5"
      },
      "outputs": [],
      "source": [
        "def sinusoidal_embedding(n, d):\n",
        "    \"\"\"\n",
        "    Returns the sinusoidal positional embedding matrix.\n",
        "\n",
        "    Args:\n",
        "        n (int): Length of sequence.\n",
        "        d (int): Dimension of the embedding.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Sinusoidal positional embedding matrix of shape (n, d).\n",
        "    \"\"\"\n",
        "    positions = torch.arange(0, n).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d, 2) * (-np.log(10000.0) / d))\n",
        "    embedding = torch.zeros(n, d)\n",
        "    embedding[:, 0::2] = torch.sin(positions * div_term)\n",
        "    embedding[:, 1::2] = torch.cos(positions * div_term)\n",
        "    return embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cc208c9e",
      "metadata": {
        "id": "cc208c9e"
      },
      "outputs": [],
      "source": [
        "class double_conv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(double_conv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class down_layer(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(down_layer, self).__init__()\n",
        "        self.pool = nn.MaxPool2d(2, stride=2, padding=0)\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(self.pool(x))\n",
        "        return x\n",
        "\n",
        "class up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(up, self).__init__()\n",
        "        self.up_scale = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)\n",
        "\n",
        "    def forward(self, x1, x2): # x1 (bs,out_ch,w1,h1) x2 (bs,in_ch,w2,h2)\n",
        "        x2 = self.up_scale(x2) # (bs,out_ch,2*w2,2*h2)\n",
        "        diffY = x1.size()[2] - x2.size()[2]\n",
        "        diffX = x1.size()[3] - x2.size()[3]\n",
        "\n",
        "        x2 = F.pad(x2, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2]) # (bs,out_ch,w1,h1)\n",
        "        x = torch.cat([x2, x1], dim=1) # (bs,2*out_ch,w1,h1)\n",
        "        return x\n",
        "\n",
        "class up_layer(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch): # !! 2*out_ch = in_ch !!\n",
        "        super(up_layer, self).__init__()\n",
        "        self.up = up(in_ch, out_ch)\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x1, x2): # x1 (bs,out_ch,w1,h1) x2 (bs,in_ch,w2,h2)\n",
        "        a = self.up(x1, x2) # (bs,2*out_ch,w1,h1)\n",
        "        x = self.conv(a) # (bs,out_ch,w1,h1) because 2*out_ch = in_ch\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1efeea0c",
      "metadata": {
        "id": "1efeea0c"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, n_steps=1000, time_emb_dim=100):\n",
        "        super(UNet, self).__init__()\n",
        "        self.conv1 = double_conv(in_channels, 64)\n",
        "        self.down1 = down_layer(64, 128)\n",
        "        self.down2 = down_layer(128, 256)\n",
        "        self.down3 = down_layer(256, 512)\n",
        "        self.down4 = down_layer(512, 1024)\n",
        "        self.up1 = up_layer(1024, 512)\n",
        "        self.up2 = up_layer(512, 256)\n",
        "        self.up3 = up_layer(256, 128)\n",
        "        self.up4 = up_layer(128, 64)\n",
        "        self.last_conv = nn.Conv2d(64, in_channels, 1)\n",
        "\n",
        "        # Time embedding\n",
        "        self.time_embed = nn.Embedding(n_steps, time_emb_dim)\n",
        "        self.time_embed.weight.data = sinusoidal_embedding(n_steps, time_emb_dim)\n",
        "        self.time_embed.requires_grad_(False)\n",
        "        self.te1 = self._make_te(time_emb_dim, in_channels)\n",
        "        self.te2 = self._make_te(time_emb_dim, 64)\n",
        "        self.te3 = self._make_te(time_emb_dim, 128)\n",
        "        self.te4 = self._make_te(time_emb_dim, 256)\n",
        "        self.te5 = self._make_te(time_emb_dim, 512)\n",
        "        self.te1_up = self._make_te(time_emb_dim, 1024)\n",
        "        self.te2_up = self._make_te(time_emb_dim, 512)\n",
        "        self.te3_up = self._make_te(time_emb_dim, 256)\n",
        "        self.te4_up = self._make_te(time_emb_dim, 128)\n",
        "\n",
        "    def _make_te(self, dim_in, dim_out):\n",
        "        return nn.Sequential(nn.Linear(dim_in, dim_out), nn.SiLU(), nn.Linear(dim_out, dim_out))\n",
        "\n",
        "    def forward(self, x , t): # x (bs,in_channels,w,d)\n",
        "        bs = x.shape[0]\n",
        "        t = self.time_embed(t)\n",
        "        x1 = self.conv1(x+self.te1(t).reshape(bs, -1, 1, 1)) # (bs,64,w,d)\n",
        "        x2 = self.down1(x1+self.te2(t).reshape(bs, -1, 1, 1)) # (bs,128,w/2,d/2)\n",
        "        x3 = self.down2(x2+self.te3(t).reshape(bs, -1, 1, 1)) # (bs,256,w/4,d/4)\n",
        "        x4 = self.down3(x3+self.te4(t).reshape(bs, -1, 1, 1)) # (bs,512,w/8,h/8)\n",
        "        x5 = self.down4(x4+self.te5(t).reshape(bs, -1, 1, 1)) # (bs,1024,w/16,h/16)\n",
        "        x1_up = self.up1(x4, x5+self.te1_up(t).reshape(bs, -1, 1, 1)) # (bs,512,w/8,h/8)\n",
        "        x2_up = self.up2(x3, x1_up+self.te2_up(t).reshape(bs, -1, 1, 1)) # (bs,256,w/4,h/4)\n",
        "        x3_up = self.up3(x2, x2_up+self.te3_up(t).reshape(bs, -1, 1, 1)) # (bs,128,w/2,h/2)\n",
        "        x4_up = self.up4(x1, x3_up+self.te4_up(t).reshape(bs, -1, 1, 1)) # (bs,64,w,h)\n",
        "        output = self.last_conv(x4_up) # (bs,in_channels,w,h)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d4c37c98",
      "metadata": {
        "id": "d4c37c98"
      },
      "outputs": [],
      "source": [
        "class DDPM(nn.Module):\n",
        "    def __init__(self, network, num_timesteps, beta_start=0.0001, beta_end=0.02, device=device) -> None:\n",
        "        super(DDPM, self).__init__()\n",
        "        self.num_timesteps = num_timesteps\n",
        "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps, dtype=torch.float32).to(device)\n",
        "        self.alphas = 1.0 - self.betas\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n",
        "        self.network = network\n",
        "        self.device = device\n",
        "        self.sqrt_alphas_cumprod = self.alphas_cumprod ** 0.5 # used in add_noise\n",
        "        self.sqrt_one_minus_alphas_cumprod = (1 - self.alphas_cumprod) ** 0.5 # used in add_noise and step\n",
        "\n",
        "    def add_noise(self, x_start, x_noise, timesteps):\n",
        "        # The forward process\n",
        "        # x_start and x_noise (bs, n_c, w, d)\n",
        "        # timesteps (bs)\n",
        "        s1 = self.sqrt_alphas_cumprod[timesteps] # bs\n",
        "        s2 = self.sqrt_one_minus_alphas_cumprod[timesteps] # bs\n",
        "        s1 = s1.reshape(-1,1,1,1) # (bs, 1, 1, 1) for broadcasting\n",
        "        s2 = s2.reshape(-1,1,1,1) # (bs, 1, 1, 1)\n",
        "        return s1 * x_start + s2 * x_noise\n",
        "\n",
        "    def reverse(self, x, t):\n",
        "        # The network return the estimation of the noise we added\n",
        "        return self.network(x, t)\n",
        "\n",
        "    def step(self, model_output, timestep, sample):\n",
        "        # one step of sampling\n",
        "        # timestep (1)\n",
        "        t = timestep\n",
        "        coef_epsilon = (1-self.alphas)/self.sqrt_one_minus_alphas_cumprod\n",
        "        coef_eps_t = coef_epsilon[t].reshape(-1,1,1,1)\n",
        "        coef_first = 1/self.alphas ** 0.5\n",
        "        coef_first_t = coef_first[t].reshape(-1,1,1,1)\n",
        "        pred_prev_sample = coef_first_t*(sample-coef_eps_t*model_output)\n",
        "\n",
        "        variance = 0\n",
        "        if t > 0:\n",
        "            noise = torch.randn_like(model_output).to(self.device)\n",
        "            variance = ((self.betas[t] ** 0.5) * noise)\n",
        "\n",
        "        pred_prev_sample = pred_prev_sample + variance\n",
        "\n",
        "        return pred_prev_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6463bad",
      "metadata": {},
      "source": [
        "## Training pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0282585f",
      "metadata": {
        "id": "0282585f"
      },
      "outputs": [],
      "source": [
        "def show_and_save_images(images, title=\"\", save_path=\"output.png\"):\n",
        "    \"\"\"\n",
        "    Displays images as subplots in a square grid and saves the figure.\n",
        "\n",
        "    Args:\n",
        "        images (list): List of images to be displayed.\n",
        "        title (str): Title of the figure.\n",
        "        save_path (str): File path to save the figure.\n",
        "    \"\"\"\n",
        "    images = [np.clip(im.permute(1, 2, 0).numpy(), 0, 1) for im in images]\n",
        "\n",
        "    num_images = len(images)\n",
        "    rows = int(np.sqrt(num_images))\n",
        "    cols = (num_images + rows - 1) // rows  # Ensure all images fit in the grid\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(8, 8))\n",
        "\n",
        "    for ax, image in zip(axes.flatten(), images):\n",
        "        ax.imshow(image)\n",
        "        ax.axis('off')\n",
        "\n",
        "    fig.suptitle(title, fontsize=30)\n",
        "    plt.savefig(save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0bec9075",
      "metadata": {
        "id": "0bec9075"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ef4f6222",
      "metadata": {
        "id": "ef4f6222"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, dataloader, optimizer, num_epochs, num_timesteps, gradient_accumulation_steps=1, device=device):\n",
        "    \"\"\"Training loop for DDPM\"\"\"\n",
        "\n",
        "    global_step = 0\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        progress_bar = tqdm(total=len(dataloader))\n",
        "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "        accumulated_steps = 0\n",
        "\n",
        "        for step, batch in enumerate(dataloader):\n",
        "            batch = batch[0].to(device)\n",
        "            noise = torch.randn(batch.shape).to(device)\n",
        "            timesteps = torch.randint(0, num_timesteps, (batch.shape[0],)).long().to(device)\n",
        "\n",
        "            noisy = model.add_noise(batch, noise, timesteps)\n",
        "            noise_pred = model.reverse(noisy, timesteps)\n",
        "            loss = F.mse_loss(noise_pred, noise)\n",
        "            loss /= gradient_accumulation_steps\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            accumulated_steps += 1\n",
        "\n",
        "            if accumulated_steps % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "            losses.append(loss.detach().item())\n",
        "            progress_bar.update(1)\n",
        "            logs = {\"loss\": loss.detach().item(), \"step\": global_step}\n",
        "            progress_bar.set_postfix(**logs)\n",
        "\n",
        "        # Perform a final optimization step for the remaining accumulated gradients\n",
        "        if accumulated_steps % gradient_accumulation_steps != 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "\n",
        "        torch.save(model.state_dict(), 'ddpm.pt')\n",
        "\n",
        "        progress_bar.close()\n",
        "\n",
        "    return losses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c849d729",
      "metadata": {},
      "source": [
        "## Dataset loading and  Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f42c4b53",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f42c4b53",
        "outputId": "fe4312f7-45f8-4c40-b7fa-5db15f00eb26"
      },
      "outputs": [],
      "source": [
        "!mkdir data_faces && wget https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "sZGhVFwPxCjH",
      "metadata": {
        "id": "sZGhVFwPxCjH"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile(\"celeba.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"data_faces/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "oZtRc-dwxEzX",
      "metadata": {
        "id": "oZtRc-dwxEzX"
      },
      "outputs": [],
      "source": [
        "transform = Compose([\n",
        "    Resize((64,64)),\n",
        "    ToTensor()]\n",
        ")\n",
        "\n",
        "batch_size = 128\n",
        "celeba_dataset = datasets.ImageFolder('data_faces/', transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "N5MRyKMbxPtD",
      "metadata": {
        "id": "N5MRyKMbxPtD"
      },
      "outputs": [],
      "source": [
        "celeba_loader = DataLoader(celeba_dataset, batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "686e60af",
      "metadata": {},
      "source": [
        "## Training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fec1b58",
      "metadata": {
        "id": "6fec1b58"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "num_epochs = 10\n",
        "num_timesteps = 1000\n",
        "network = UNet(in_channels=3)\n",
        "network.to(device)\n",
        "model = DDPM(network, num_timesteps, beta_start=0.0001, beta_end=0.02, device=device)\n",
        "model.train()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "training_loop(model, celeba_loader, optimizer, num_epochs, num_timesteps, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcfabf6a",
      "metadata": {},
      "source": [
        "## Sampling images from the target distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "8a2cd2e3",
      "metadata": {
        "id": "8a2cd2e3"
      },
      "outputs": [],
      "source": [
        "def generate_image(ddpm, sample_size, channel, size, device):\n",
        "    \"\"\"\n",
        "    Generate the image from the Gaussian noise.\n",
        "\n",
        "    Args:\n",
        "        ddpm (torch.nn.Module): DDPM model.\n",
        "        sample_size (int): Number of samples to generate.\n",
        "        channel (int): Number of channels.\n",
        "        size (int): Size of the image.\n",
        "        device (str): Device to run the generation on.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Tuple containing two lists of generated frames.\n",
        "    \"\"\"\n",
        "    frames = []\n",
        "    frames_mid = []\n",
        "    ddpm.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        timesteps = list(range(ddpm.num_timesteps))[::-1]\n",
        "        sample = torch.randn(sample_size, channel, size, size).to(device)\n",
        "\n",
        "        for i, t in enumerate(tqdm(timesteps)):\n",
        "            time_tensor = torch.full((sample_size,), t, dtype=torch.long).to(device)\n",
        "            residual = ddpm.reverse(sample, time_tensor)\n",
        "            sample = ddpm.step(residual, time_tensor[0], sample)\n",
        "\n",
        "            if t == 500:\n",
        "                for i in range(sample_size):\n",
        "                    frames_mid.append(sample[i].detach().cpu())\n",
        "\n",
        "        for i in range(sample_size):\n",
        "            frames.append(sample[i].detach().cpu())\n",
        "\n",
        "    return frames, frames_mid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2ac7e7b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365,
          "referenced_widgets": [
            "86948e63de0c4391ae82bf31ecc8a94a",
            "84130690cc094b37a9276136af951af1",
            "0644c4f65c59421ab02c70d8970a194e",
            "cf48e1884af64fd6a481049ade145f0c",
            "c11158773a8842428f329304d8e30e65",
            "e58e18d8d7384ea78948b131f0f47808",
            "fe09be672ea949878b5939988e0cfb3c",
            "6f468cb906194997913a43b99410969e",
            "76f86a0c692742dc9025faf1fa19635d",
            "eb75774242054771a963bfb2203666f0",
            "2682d93b5d7846faa332bec932b43c87"
          ]
        },
        "id": "f2ac7e7b",
        "outputId": "f2a2850f-aeee-4aa6-a23f-ab67eabc0e1a"
      },
      "outputs": [],
      "source": [
        "generated, generated_mid = generate_image(model, 10000, 3, 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f184f89b",
      "metadata": {
        "id": "f184f89b"
      },
      "outputs": [],
      "source": [
        "show_and_save_images(generated, \"Final result\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l9Yfn_nB6pbu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "l9Yfn_nB6pbu",
        "outputId": "f7c1ac86-36dd-4b46-a8a6-b740403f0e4e"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the file\n",
        "files.download('ddpm.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q723sipTOfkm",
      "metadata": {
        "id": "q723sipTOfkm"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('ddpm.pt', map_location=device))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccdaac83",
      "metadata": {},
      "source": [
        "## Measuring performances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NGP8R1jZDgCX",
      "metadata": {
        "id": "NGP8R1jZDgCX"
      },
      "outputs": [],
      "source": [
        "def calculate_activation_statistics(loader, model, device):\n",
        "    model.eval()\n",
        "    features = []\n",
        "    with torch.no_grad():\n",
        "        for images in loader:\n",
        "            images = images[0].to(device)\n",
        "            activations = model(images)\n",
        "            features.append(activations)\n",
        "    features = torch.cat(features, dim=0)\n",
        "    mu = torch.mean(features, dim=0)\n",
        "    sigma = torch.matmul((features - mu).T, (features - mu)) / (features.size(0) - 1)\n",
        "    return mu, sigma\n",
        "\n",
        "def calculate_activation_statistics_gen(loader, model, device):\n",
        "    model.eval()\n",
        "    features = []\n",
        "    with torch.no_grad():\n",
        "        for images in loader:\n",
        "            images = images.to(device)\n",
        "            activations = model(images)\n",
        "            features.append(activations)\n",
        "    features = torch.cat(features, dim=0)\n",
        "    mu = torch.mean(features, dim=0)\n",
        "    sigma = torch.matmul((features - mu).T, (features - mu)) / (features.size(0) - 1)\n",
        "    return mu, sigma\n",
        "\n",
        "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2):\n",
        "    eps = 1e-6\n",
        "    diff = mu1 - mu2\n",
        "    covmean = sqrtm(sigma1 @ sigma2)\n",
        "    fid = (diff.dot(diff) + torch.trace(sigma1 + sigma2 - 2 * covmean)).real\n",
        "    return fid.item()\n",
        "\n",
        "def compute_fid_score(real_loader, generated_loader):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load resnet18 model pre-trained on ImageNet\n",
        "    net = models.resnet18(weights='IMAGENET1K_V1').to(device)\n",
        "    net.eval()\n",
        "\n",
        "    # Compute statistics for real and generated images\n",
        "    real_mu, real_sigma = calculate_activation_statistics(real_loader, net, device)\n",
        "    generated_mu, generated_sigma = calculate_activation_statistics_gen(generated_loader, net, device)\n",
        "\n",
        "    # Compute FID score\n",
        "    fid_score = calculate_frechet_distance(real_mu.to('cpu'), real_sigma.to('cpu'), generated_mu.to('cpu'), generated_sigma.to('cpu'))\n",
        "\n",
        "    return fid_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fc84e4a",
      "metadata": {
        "id": "5fc84e4a"
      },
      "outputs": [],
      "source": [
        "# create a dataloader for the generated images\n",
        "\n",
        "# Transformations for images\n",
        "preprocess = transforms.Compose([\n",
        "        transforms.Resize(32),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "\n",
        "generated_loader = DataLoader(generated, batch_size, shuffle=True)\n",
        "\n",
        "# take only the first 1000 images from the real dataset\n",
        "celeba_dataset = datasets.ImageFolder('data_faces/', transform=preprocess)\n",
        "celeba_dataset = torch.utils.data.Subset(celeba_dataset, list(range(10000)))\n",
        "real_loader = DataLoader(celeba_dataset, batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "fid_score = compute_fid_score(real_loader, generated_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DLW_nPxM7UWi",
      "metadata": {
        "id": "DLW_nPxM7UWi"
      },
      "outputs": [],
      "source": [
        "fid_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "104f1244",
      "metadata": {},
      "source": [
        "## Comparison with baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7085f3d",
      "metadata": {},
      "outputs": [],
      "source": [
        "class LinearMLP(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(LinearMLP, self).__init__()\n",
        "\n",
        "        # Calculate the size of the flattened input\n",
        "        flattened_size = 64 * 64 * in_channels\n",
        "\n",
        "        # Define linear layers\n",
        "        self.linear1 = nn.Linear(flattened_size, hidden_channels)\n",
        "        self.linear2 = nn.Linear(hidden_channels, hidden_channels)\n",
        "        self.linear3 = nn.Linear(hidden_channels, flattened_size)\n",
        "\n",
        "        # Define activation function\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        # Flatten the input image\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, -1)\n",
        "\n",
        "        # Pass through linear layers with activation functions\n",
        "        x = self.activation(self.linear1(x))\n",
        "        x = self.activation(self.linear2(x))\n",
        "        x = self.linear3(x)\n",
        "\n",
        "        # Reshape back to image shape\n",
        "        x = x.view(batch_size, 3, 64, 64)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e455dfc",
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "num_epochs = 10\n",
        "num_timesteps = 1000\n",
        "network_baseline = LinearMLP(in_channels = 3, hidden_channels = 100, out_channels = 3)\n",
        "network_baseline.to(device)\n",
        "model_baseline = DDPM(network_baseline, num_timesteps, beta_start=0.0001, beta_end=0.02, device=device)\n",
        "model_baseline.train()\n",
        "optimizer = torch.optim.Adam(model_baseline.parameters(), lr=learning_rate)\n",
        "training_loop(model_baseline, celeba_loader, optimizer, num_epochs, num_timesteps, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14010745",
      "metadata": {},
      "outputs": [],
      "source": [
        "generated_baseline = generate_image(model_baseline, 10000, 3, 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0462909f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transformations for images\n",
        "preprocess = transforms.Compose([\n",
        "        transforms.Resize(32),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "generated_loader = DataLoader(generated_baseline, batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "fid_score_baseline = compute_fid_score(real_loader, generated_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f528bfb2",
      "metadata": {},
      "outputs": [],
      "source": [
        "r = fid_score/fid_score_baseline\n",
        "print(r)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1f5f127",
      "metadata": {},
      "source": [
        "## Interpolation of visage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "366d3db8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def interpolate_visages(ddpm, visage1, visage2, num_interpolation_steps, device):\n",
        "    \"\"\"\n",
        "    Interpolate between two visages.\n",
        "\n",
        "    Args:\n",
        "        ddpm (torch.nn.Module): DDPM model.\n",
        "        visage1 (torch.Tensor): First visage (image).\n",
        "        visage2 (torch.Tensor): Second visage (image).\n",
        "        num_interpolation_steps (int): Number of interpolation steps between the visages.\n",
        "        device (str): Device to run the interpolation on.\n",
        "\n",
        "    Returns:\n",
        "        list: List of interpolated visages (images).\n",
        "    \"\"\"\n",
        "    ddpm.eval()\n",
        "\n",
        "    # Encode visages into noise representations\n",
        "    with torch.no_grad():\n",
        "        noise1 = ddpm.reverse(visage1.unsqueeze(0).to(device), torch.tensor([0]).to(device))\n",
        "        noise2 = ddpm.reverse(visage2.unsqueeze(0).to(device), torch.tensor([0]).to(device))\n",
        "\n",
        "        interpolated_visages = []\n",
        "\n",
        "        # Interpolate between the noise representations\n",
        "        for i in range(num_interpolation_steps + 2):  # Including endpoints\n",
        "            alpha = i / (num_interpolation_steps + 1)\n",
        "            interpolated_noise = alpha * noise2 + (1 - alpha) * noise1\n",
        "\n",
        "            # Decode interpolated noise representation back into image\n",
        "            interpolated_visage = ddpm.step(interpolated_noise, torch.tensor([0]).to(device), visage1.unsqueeze(0).to(device))\n",
        "            interpolated_visages.append(interpolated_visage.squeeze().detach().cpu())\n",
        "\n",
        "    return interpolated_visages\n",
        "\n",
        "\n",
        "visage1 = celeba_dataset[0][0].unsqueeze(0)  # First visage\n",
        "visage2 = celeba_dataset[1][0].unsqueeze(0)  # Second visage\n",
        "\n",
        "# Interpolate between the two visages\n",
        "num_interpolation_steps = 5\n",
        "interpolated_visages = interpolate_visages(model, visage1, visage2, num_interpolation_steps, device)\n",
        "\n",
        "# Plot the original visages and the interpolated visages\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Plot the first visage\n",
        "plt.subplot(1, num_interpolation_steps + 2, 1)\n",
        "plt.imshow(transforms.ToPILImage()(visage1))\n",
        "plt.title('Visage 1')\n",
        "plt.axis('off')\n",
        "\n",
        "# Plot the last visage\n",
        "plt.subplot(1, num_interpolation_steps + 2, num_interpolation_steps + 2)\n",
        "plt.imshow(transforms.ToPILImage()(visage1))\n",
        "plt.title('Visage 2')\n",
        "plt.axis('off')\n",
        "\n",
        "# Plot the interpolated visages\n",
        "for i, interpolated_visage in enumerate(interpolated_visages):\n",
        "    plt.subplot(1, num_interpolation_steps + 2, i + 2)\n",
        "    plt.imshow(transforms.ToPILImage()(interpolated_visage))\n",
        "    plt.title(f'Interpolation {i+1}')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0644c4f65c59421ab02c70d8970a194e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f468cb906194997913a43b99410969e",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_76f86a0c692742dc9025faf1fa19635d",
            "value": 0
          }
        },
        "2682d93b5d7846faa332bec932b43c87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f468cb906194997913a43b99410969e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76f86a0c692742dc9025faf1fa19635d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "84130690cc094b37a9276136af951af1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e58e18d8d7384ea78948b131f0f47808",
            "placeholder": "​",
            "style": "IPY_MODEL_fe09be672ea949878b5939988e0cfb3c",
            "value": "  0%"
          }
        },
        "86948e63de0c4391ae82bf31ecc8a94a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_84130690cc094b37a9276136af951af1",
              "IPY_MODEL_0644c4f65c59421ab02c70d8970a194e",
              "IPY_MODEL_cf48e1884af64fd6a481049ade145f0c"
            ],
            "layout": "IPY_MODEL_c11158773a8842428f329304d8e30e65"
          }
        },
        "c11158773a8842428f329304d8e30e65": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf48e1884af64fd6a481049ade145f0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb75774242054771a963bfb2203666f0",
            "placeholder": "​",
            "style": "IPY_MODEL_2682d93b5d7846faa332bec932b43c87",
            "value": " 0/1000 [00:00&lt;?, ?it/s]"
          }
        },
        "e58e18d8d7384ea78948b131f0f47808": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb75774242054771a963bfb2203666f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe09be672ea949878b5939988e0cfb3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
